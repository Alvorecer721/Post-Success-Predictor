{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Below is the final data cleaning model.\n",
    "- To use this model on your laptop, you have to modified paths to each CSV file.\n",
    "- Code for each working component is in the *Components* folder, you can use it if you don't get what this model does, as\n",
    "  well as using it for understanding, debugging and extending.\n",
    "- The first cell of code is to extract and convert post data into a dataframe:\n",
    "    * Split the single column with all the information into multiple columns with individual information.\n",
    "    * Group and re-order entire dataframe according to topic tags.\n",
    "    * Combine multiple elements of actual topic tags into one and fill the gaps where empty elements appears.\n",
    "    * Drop unwanted columns, remove garbled characters, and replace characters with digits.\n",
    "- The second cell of code is to extract and convert post impression data, details are almost the same as above, except calculating the ratio.\n",
    "- The third cell of code is to extract and convert reaction summary data.\n",
    "- The fourth cell of code is to extract and convert customer data and search data, and merge them to a single dataframe.\n",
    "- The final cell of code is to merge all the dataframes above into a single dataframe, drop unwanted columns, merge some information and generate the csv files of the final data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'UNiDAYS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a822ed4d1781>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mUNiDAYS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mYixuan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mData_Processing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyFunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# from UNiDAYS.Yixuan.Data_Processing.myFunctions import findAndReplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# from UNiDAYS.Yixuan.Data_Processing.myFunctions import combineMultipleElem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'UNiDAYS'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "from UNiDAYS.Final_Model.myFunctions import *\n",
    "# from UNiDAYS.Yixuan.Data_Processing.myFunctions import findAndReplace\n",
    "# from UNiDAYS.Yixuan.Data_Processing.myFunctions import combineMultipleElem\n",
    "# from UNiDAYS.Yixuan.Data_Processing.myFunctions import shiftElementPosMult\n",
    "# from UNiDAYS.Yixuan.Data_Processing.myFunctions import shiftElementPosSingle\n",
    "# from UNiDAYS.Yixuan.Data_Processing.myFunctions import cleanData\n",
    "# from UNiDAYS.Yixuan.Data_Processing.myFunctions import calculateRatio\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 5000)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# V1\n",
    "# Post_data_csv = r'/Volumes/UNiDAYS/postdata.csv'\n",
    "#\n",
    "# # Post impression data needs to be loaded into dataframe and store as a csv file and reload again\n",
    "# # To avoid weird bugs\n",
    "# Post_impression_data_json = r'/Volumes/UNiDAYS/EngagementData/PostImpressionsData.json'\n",
    "# # You need to specify a path to store the csv file\n",
    "# Post_impression_data_csv = r'/Users/xuyixuan/Desktop/temp1.csv'\n",
    "#\n",
    "# # Same as above\n",
    "# Reaction_summery_data_json = r'/Volumes/UNiDAYS/EngagementData/ReactionsSummaryData.json'\n",
    "# Reaction_summery_data_csv = r'/Users/xuyixuan/Desktop/temp2.csv'\n",
    "#\n",
    "# Customer_data_csv = r'/Volumes/UNiDAYS/customerdata.csv'\n",
    "# Search_data_csv = r'/Volumes/UNiDAYS/searchdata.csv'\n",
    "# Body_friName_data_csv = r'/Volumes/UNiDAYS/ProcessedData/body_and_friendly_name_analysis.csv'\n",
    "#\n",
    "# FINAL_DATA_DESTINATION = r'/Users/xuyixuan/Desktop/temp3.csv'\n",
    "\n",
    "\n",
    "\n",
    "# V2\n",
    "Post_data_csv = r'/Volumes/UNiDAYS/V2/postdata.csv'\n",
    "\n",
    "# Post impression data needs to be loaded into dataframe and store as a csv file and reload again\n",
    "# To avoid weird bugs\n",
    "Post_impression_data_json = r'/Volumes/UNiDAYS/V2/EngagementData/PostImpressionsData.json'\n",
    "# You need to specify a path to store the csv file\n",
    "Post_impression_data_csv = r'/Users/xuyixuan/Desktop/temp4.csv'\n",
    "\n",
    "# Same as above\n",
    "Reaction_summery_data_json = r'/Volumes/UNiDAYS/V2/EngagementData/ReactionsSummaryData.json'\n",
    "Reaction_summery_data_csv = r'/Users/xuyixuan/Desktop/temp5.csv'\n",
    "\n",
    "Customer_data_csv = r'/Volumes/UNiDAYS/V2/customerdata.csv'\n",
    "Search_data_csv = r'/Volumes/UNiDAYS/V2/searchdata.csv'\n",
    "Body_friName_data_csv = r'/Volumes/UNiDAYS/ProcessedData/body_and_friendly_name_analysis.csv'\n",
    "\n",
    "FINAL_DATA_DESTINATION = r'/Users/xuyixuan/Desktop/temp6.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 144: expected 9 fields, saw 22\\nSkipping line 146: expected 9 fields, saw 22\\n'\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(Post_data_csv, error_bad_lines=False)\n",
    "# data = data[\"data\"].str.split(', \"', n = -1, expand = True)\n",
    "data = data[data.columns[1]].str.split(', \"', n = -1, expand = True)\n",
    "data = data.loc[data[2] == 'PostType\": 1']\n",
    "# only keep postType = 1\n",
    "\n",
    "# separate data with actual topicTags into one data frame\n",
    "data_actual_topicTags = data.loc[(data[5] != 'TopicTags\": null') & (data[5] != 'TopicTags\": []')]\n",
    "# separate data with null or empty topicTags into another data frame\n",
    "data_empty_topicTags = data.loc[(data[5] == 'TopicTags\": null') | (data[5] == 'TopicTags\": []')]\n",
    "\n",
    "# combine all topic tags into one element\n",
    "combineMultipleElem(data_actual_topicTags, 5, ']', 5)\n",
    "# switch columns from back to the front\n",
    "shiftElementPosMult(data_actual_topicTags, 6, 'VersionId')\n",
    "# concatenate dataframe with actual topic tags with dataframe with empty topic tags to reform the original dataframe\n",
    "post_data = pd.concat([data_actual_topicTags, data_empty_topicTags])\n",
    "\n",
    "# fill the gap\n",
    "for row in range(len(post_data)):\n",
    "    shiftElementPosSingle(post_data, row, 7, \"CreativeId\")\n",
    "    shiftElementPosSingle(post_data, row, 16, 'Title\": ')\n",
    "\n",
    "post_data.drop(post_data.iloc[:, 25:33], inplace = True, axis = 1)\n",
    "post_data.drop(post_data.columns[[2, 3, 4, 6, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 23, 24]], axis = 1, inplace= True)\n",
    "post_data = post_data.reset_index(drop=True)\n",
    "# reset row index\n",
    "post_data.columns = range(post_data.shape[1])\n",
    "# reset column index\n",
    "\n",
    "# transform country code into numbers\n",
    "for row in range(len(post_data)):\n",
    "    findAndReplace(post_data, post_data, \"US\", \"1\", row, 4, 4)  # United State\n",
    "    findAndReplace(post_data, post_data, \"CA\", \"2\", row, 4, 4)  # Canada\n",
    "    findAndReplace(post_data, post_data, \"IE\", \"3\", row, 4, 4)  # Ireland\n",
    "    findAndReplace(post_data, post_data, \"GB\", \"4\", row, 4, 4)  # Great Britain\n",
    "    findAndReplace(post_data, post_data, \"AU\", \"5\", row, 4, 4)  # Australia\n",
    "    findAndReplace(post_data, post_data, \"NZ\", \"6\", row, 4, 4)  # New Zealand\n",
    "    findAndReplace(post_data, post_data, \"NL\", \"7\", row, 4, 4)  # Netherlands\n",
    "    findAndReplace(post_data, post_data, \"IT\", \"8\", row, 4, 4)  # Italy\n",
    "    findAndReplace(post_data, post_data, \"DK\", \"9\", row, 4, 4)  # Denmark\n",
    "    findAndReplace(post_data, post_data, \"AT\", \"10\", row, 4, 4) # Austria\n",
    "    findAndReplace(post_data, post_data, \"DE\", \"11\", row, 4, 4) # Germany\n",
    "    findAndReplace(post_data, post_data, \"BE\", \"12\", row, 4, 4) # Belgium\n",
    "    findAndReplace(post_data, post_data, \"CH\", \"13\", row, 4, 4) # Swiss\n",
    "    findAndReplace(post_data, post_data, \"L\", \"14\", row, 4, 4)  # Luxembourg\n",
    "    findAndReplace(post_data, post_data, \"FR\", \"15\", row, 4, 4) # France\n",
    "    findAndReplace(post_data, post_data, \"FI\", \"16\", row, 4, 4) # Finland\n",
    "    findAndReplace(post_data, post_data, \"SG\", \"17\", row, 4, 4) # Singapore\n",
    "    findAndReplace(post_data, post_data, \"ES\", \"18\", row, 4, 4) # Spain\n",
    "    findAndReplace(post_data, post_data, \"E\", \"18\", row, 4, 4)  # Spain\n",
    "    findAndReplace(post_data, post_data, \"O\", \"19\", row, 4, 4)  # Unknown\n",
    "    findAndReplace(post_data, post_data, \"G\", \"20\", row, 4, 4)  # Unknown\n",
    "\n",
    "# remove garbled characters\n",
    "post_data[0] = post_data[0].str.lstrip('{\"')\n",
    "post_data[2] = post_data[2].str.replace('\"', '')\n",
    "\n",
    "cleanData(post_data, string.ascii_letters, ' \"[]:SN')\n",
    "post_data.columns = ['Post Id', 'CanReact', 'Topic Tags', 'Customer Id', 'Cultural Code', 'Is Published', 'Publish Data', 'Last Modified On', 'Customer Region Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read json file into dataframe with column contains actual information, and store it in a csv file\n",
    "data = pd.read_json(Post_impression_data_json)\n",
    "data = data[\"Items\"]\n",
    "data.to_csv(Post_impression_data_csv)\n",
    "\n",
    "# read csv file into dataframe, if not do so will cause weired error\n",
    "data = pd.read_csv(Post_impression_data_csv)\n",
    "data = data[\"Items\"].str.split(\", '\", n = -1, expand = True)\n",
    "\n",
    "# remove unnecessary characters\n",
    "data[1] = data[1].str.lstrip(\"engagement_breakdown': \")\n",
    "data[1] = data[1].str.lstrip(\"{'M': {'\")\n",
    "\n",
    "impression_data = pd.DataFrame(columns = ['Post Id', 'Impressions', 'Engagements', 'Partner Header Clicked', 'Content Card Clicked', 'Shoppable Clicked', 'Content Card Poll Completed'])\n",
    "impression_data['Impressions'] = data[0]\n",
    "\n",
    "for row in range (len(data)):\n",
    "    for column in range(1, len(data.columns)):\n",
    "        findAndReplace(data, impression_data, 'post_id', data.iat[row, column], row, column, 0)\n",
    "        findAndReplace(data, impression_data, 'engagements', data.iat[row, column], row, column, 2)\n",
    "        findAndReplace(data, impression_data, 'partner_header_clicked', data.iat[row, column], row, column, 3)\n",
    "        findAndReplace(data, impression_data, 'content_card_clicked', data.iat[row, column], row, column, 4)\n",
    "        findAndReplace(data, impression_data, 'shoppable_clicked', data.iat[row, column], row, column, 5)\n",
    "        findAndReplace(data, impression_data, 'content_card_poll_completed', data.iat[row, column], row, column, 6)\n",
    "        \n",
    "cleanData(impression_data, string.ascii_letters + \"_{ '\", \" '{}:SN\")\n",
    "\n",
    "# initialise the Ratio column, calculate the result and put it in the Ratio column\n",
    "impression_data['Ratio'] = 'default value'\n",
    "for row in range(len(impression_data)):\n",
    "    impression_data.iat[row, 7] = calculateRatio(int(impression_data.iat[row, 1]), int(impression_data.iat[row, 2]))\n",
    "\n",
    "impression_data.drop(impression_data.iloc[:, 3:7], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_json(Reaction_summery_data_json)\n",
    "data = data['Items']\n",
    "data.to_csv(Reaction_summery_data_csv)\n",
    "data = pd.read_csv(Reaction_summery_data_csv)\n",
    "data = data['Items'].str.split(\", '\", n = -1, expand = True)\n",
    "\n",
    "data[0] = data[0].str.lstrip(\"{'reactions': \")\n",
    "data[0] = data[0].str.lstrip(\"{'M': {'\")\n",
    "\n",
    "reaction_data = pd.DataFrame(columns = ['Post Id', 'Yeah!', 'Lit', 'Meh', 'Huh', 'Yikes'])\n",
    "reaction_data['Post Id'] = data[0]\n",
    "\n",
    "for row in range (len(data)):\n",
    "    for column in range(len(data.columns)):\n",
    "        findAndReplace(data, reaction_data, \"post_id\", data.iat[row, column],row, column, 0)\n",
    "        findAndReplace(data, reaction_data, \"r5\", data.iat[row, column], row, column, 1)\n",
    "        findAndReplace(data, reaction_data, \"r4\", data.iat[row, column], row, column, 2)\n",
    "        findAndReplace(data, reaction_data, \"r3\", data.iat[row, column], row, column, 3)\n",
    "        findAndReplace(data, reaction_data, \"r2\", data.iat[row, column], row, column, 4)\n",
    "        findAndReplace(data, reaction_data, \"r1\", data.iat[row, column], row, column, 5)\n",
    "        \n",
    "cleanData(reaction_data, string.ascii_letters + \"_{ '012345\", \" '{}:SN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "customer_data = pd.read_csv(Customer_data_csv, header=None, names=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k'])\n",
    "search_data = pd.read_csv(Search_data_csv, header=None, names=['Customer', 'Search Volumn'])\n",
    "search_data.to_csv(r'/Users/xuyixuan/Desktop/check0.csv')\n",
    "\n",
    "combineMultipleElem(customer_data, 2, ']', 2)\n",
    "\n",
    "data = pd.DataFrame(columns=['Customer Region Id', 'Customer', 'Category'])\n",
    "data['Customer Region Id'] = customer_data['a']\n",
    "data['Customer'] = customer_data['b']\n",
    "data['Category'] = customer_data['c']\n",
    "\n",
    "data['Customer'] = data['Customer'].str.lstrip(string.ascii_letters + ' ')\n",
    "search_data['Customer'] = search_data['Customer'].str.lstrip(string.ascii_letters + ' ')\n",
    "\n",
    "combine_search_and_customer_data = pd.merge(data, search_data, on='Customer')\n",
    "combine_search_and_customer_data.to_csv(r'/Users/xuyixuan/Desktop/check.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import csv\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "Body_friName_data_csv = r'/Volumes/UNiDAYS/V2/result.csv'\n",
    "\n",
    "def is_emoji(str):\n",
    "    \"\"\"\n",
    "    check if a string contains emoji\n",
    "    :param str: the tested string\n",
    "    :return: the number of emoji the string have\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for emoji in UNICODE_EMOJI:\n",
    "        count += str.count(emoji)\n",
    "    return int(count)\n",
    "\n",
    "\n",
    "with open(Post_data_csv, newline='', encoding='ISO8859') as csv_file:\n",
    "    with open(Body_friName_data_csv, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Post Id\", \"FCompound\", \"FIfContain%\", \"FIfContainCustomer\", \"FIfContainDollors\", \"FIfContainEmoji\",\n",
    "                         \"BodyCompound\", \"BodyIfContain%\", \"BodyIfContainCustomer\", \"BodyIfContainDollors\",\n",
    "                         \"BodyIfContainEmoji\"])\n",
    "\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                line_count += 1\n",
    "            elif not len(row):\n",
    "                line_count += 1\n",
    "            else:\n",
    "                str1 = \"FriendlyName\"\n",
    "                str2 = \"LastUpdatedBy\"\n",
    "                Fbegin = row[1].index(str1) + len('FriendlyName\": \"')\n",
    "                Fend = row[1].index(str2) - len('\", \"')\n",
    "                # friendly name\n",
    "                str3 = row[1][Fbegin:Fend]\n",
    "                sid = SentimentIntensityAnalyzer()\n",
    "                ss = sid.polarity_scores(str3)\n",
    "                judge1 = \"%\" in str3\n",
    "                judge2 = \"Customer\" in str3\n",
    "                judgeFDollors = \"$\" in str3\n",
    "                judgeFEmoji = is_emoji(str3) >= 1\n",
    "\n",
    "                str4 = '\"Body\"'\n",
    "                str5 = \"Link\"\n",
    "                str6 = \"$type\"\n",
    "                if str4 in row[1]:\n",
    "                    Bodybegin = row[1].index(str4) + len('\"Body\": \"')\n",
    "\n",
    "                    if row[1].index(str6) < row[1].index(str5):\n",
    "                        Bodyend = row[1].index(str6) - len('\", \"')\n",
    "                    else:\n",
    "                        Bodyend = row[1].index(str5) - len('\", \"')\n",
    "\n",
    "                    #body\n",
    "                    strBody = row[1][Bodybegin:Bodyend]\n",
    "                    if strBody == '':\n",
    "                        writer.writerow(\n",
    "                            [row[0], ss['compound'], judge1, judge2, judgeFDollors, judgeFEmoji, \"NULL\", \"NULL\", \"NULL\",\n",
    "                             \"NULL\", \"NULL\"])\n",
    "                    else:\n",
    "                        judge3 = \"%\" in strBody\n",
    "                        judge4 = \"Customer\" in strBody\n",
    "                        judgeBodyEmoji = is_emoji(strBody) >= 1\n",
    "                        judgeBodyDolllors = \"$\" in strBody\n",
    "                        ss1 = sid.polarity_scores(strBody)\n",
    "                        writer.writerow(\n",
    "                            [row[0], ss['compound'], judge1, judge2, judgeFDollors, judgeFEmoji, ss1['compound'],\n",
    "                             judge3, judge4, judgeBodyDolllors, judgeBodyEmoji])\n",
    "                else:\n",
    "                    writer.writerow(\n",
    "                        [row[0], ss['compound'], judge1, judge2, judgeFDollors, judgeFEmoji, \"NULL\", \"NULL\", \"NULL\",\n",
    "                         \"NULL\", \"NULL\"])\n",
    "\n",
    "\n",
    "print (\"write over\")      \n",
    "\n",
    "body_friName_data = pd.read_csv(Body_friName_data_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store the result into body_friName_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_data = pd.merge(post_data, combine_search_and_customer_data, on='Customer Region Id')\n",
    "final_data = pd.merge(final_data, impression_data, on='Post Id')\n",
    "final_data = pd.merge(final_data, reaction_data, on='Post Id')\n",
    "final_data = pd.merge(final_data, body_friName_data, on='Post Id')\n",
    "\n",
    "# since friendly name and body are both text, and they have basically the same features\n",
    "# therefore we can combine same feature of friendly name and body together\n",
    "# combination rule is, if any of the feature is true, then combined one is true, else combined one is false\n",
    "for row in range(len(final_data)):\n",
    "    if final_data.loc[row, 'FIfContain%'] != final_data.loc[row, 'BodyIfContain%']:\n",
    "        final_data.loc[row, 'FIfContain%'] = True\n",
    "    if final_data.loc[row, 'FIfContainCustomer'] != final_data.loc[row, 'BodyIfContainCustomer']:\n",
    "        final_data.loc[row, 'FIfContainCustomer'] = True\n",
    "    if final_data.loc[row, 'FIfContainDollors'] != final_data.loc[row, 'BodyIfContainDollors']:\n",
    "        final_data.loc[row, 'FIfContainDollors'] = True\n",
    "    if final_data.loc[row, 'FIfContainEmoji'] != final_data.loc[row, 'BodyIfContainEmoji']:\n",
    "        final_data.loc[row, 'FIfContainEmoji'] = True\n",
    "\n",
    "final_data = final_data.drop(['CanReact', 'Customer Id', 'Is Published', 'Publish Data', 'Last Modified On',\n",
    "                    'Customer Region Id', 'Customer', 'FCompound', 'BodyIfContain%', 'BodyIfContainCustomer', 'BodyIfContainDollors',\n",
    "                    'BodyIfContainEmoji', 'Topic Tags', 'Category'], axis=1)\n",
    "\n",
    "final_data = final_data.rename(columns={'FIfContain%': 'IfContain%', 'FIfContainCustomer': 'IfContainCustomer',\n",
    "                              'FIfContainDollors': 'IfContainDollers', 'FIfContainEmoji': 'IfContainEmoji'})\n",
    "\n",
    "final_data[[\"Yeah!\", \"Lit\", \"Meh\", \"Huh\", \"Yikes\"]].replace(\"\", numpy.NaN)\n",
    "final_data.fillna(0, inplace=True)\n",
    "\n",
    "final_data.to_csv(FINAL_DATA_DESTINATION)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}